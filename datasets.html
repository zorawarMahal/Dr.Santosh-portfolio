<html coupert-item="9AF8D9A4E502F3784AD24272D81F0381"><head>
    <title>Datasets</title>
    <meta http-equiv="content-type" content="text/html;charset=utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">  
    <meta name="description" content="Dr. Shiv Ram Dubey's home page.">
    <meta name="author" content="Dr. Shiv Ram Dubey, IIIT Allahabad">
    <meta name="keywords" content="Shiv Ram Dubey, Dr. Shiv Ram Dubey, Computer Vision, Deep Learning, CVBL, IIITA, IIIT Allahabad, IIIT-Allahabad, IIIT Sri City, IIITS.">
  
    <!-- Bootstrap -->
    <!--<script type="text/javascript" src="./javascripts/jquery.min.js"></script>
    <script type="text/javascript" src="./javascripts/bootstrap.min.js"></script>-->
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css">
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.2.1/jquery.min.js"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js"></script>
  
    <!-- Stylesheets -->
    <link rel="stylesheet" href="./css/syntax.css">
    <link rel="stylesheet" href="./css/main.css">
    <link rel="canonical" href="https://profile.iiita.ac.in/srdubey/">
    <link rel="alternate" type="application/rss+xml" title="Dr. Shiv Ram Dubey" href="https://profile.iiita.ac.in/srdubey/">
  
    <!-- Javascript -->  
    <script src="./javascripts/hidebib.js" type="text/javascript"></script>
  
    <!-- Favicon -->
    <link rel="icon" href="./images/srd_ico.jpg" type="image/x-icon">
    <link rel="shortcut icon" href="./images/srd_ico.jpg" type="image/x-icon">
  
    <!-- Google Analytics -->
    <!-- <script type="text/javascript">
      var _gaq = _gaq || [];
      _gaq.push(['_setAccount', 'UA-818304-1']);
      _gaq.push(['_trackPageview']);
  
      (function() {
      var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
      ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
      var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
      })();
    </script>-->
  
    <!-- Google custom search -->
    <!-- <script type="text/javascript">
     (function() {
       var cx = '8ec158c7042541361';
       var gcse = document.createElement('script');
       gcse.type = 'text/javascript';
       gcse.async = true;
       gcse.src = 'https://cse.google.com/cse.js?cx=' + cx;
       var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(gcse, s);
     })();
    </script>-->
    <!-- <script async src="https://cse.google.com/cse.js?cx=8ec158c7042541361"></script>
    <div class="gcse-search"></div> -->
  </head>
  
  
  <body data-new-gr-c-s-check-loaded="14.1193.0" data-gr-ext-installed="">
  
  <nav class="navbar navbar-inverse navbar-static-top">
    <div class="container" style="width:95%;">
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand" href="./index.html">Dr. Santosh Kumar</a>
      </div>
      
      <div id="navbar" class="collapse navbar-collapse">
        <ul class="nav navbar-nav">
          <li><a href="./index.html">Home</a></li>
          <li><a href="./research.html">Research</a></li>
          <li><a href="./publications.html">Publications</a></li>
          <li><a href="./students.html">Students</a></li>
          <li><a href="./teaching.html">Teaching</a></li>
          <li><a href="./activities.html">Professional Activities</a></li>
          <li><a href="./datasets.html">Datasets</a></li>
        </ul>
      </div><!--/.nav-collapse -->
    </div>
  </nav>
  
  <div class="container" style="width:95%;">
  <h1>Datasets</h1>
  <br>
  <ul>
      <li><h5><strong>Wild Selfie Dataset (WSD)</strong></h5>
          <p> The Wild Selfie Dataset (WSD) contains the selfie images captured from the cameras of different smart phones, 
          unlike existing datasets where most of the images are captured in controlled environment. 
          The WSD dataset contains 45,424 images from 42 individuals (i.e., 24 female and 18 male subjects), 
          which are divided into 40,862 training and 4,562 test images. 
          The average number of images per subject is 1,082 with minimum and maximum number of images for any subject are 518 and 2,634, respectively. 
          The proposed dataset consists of several challenges, including but not limited to augmented reality filtering, 
          mirrored images, occlusion, illumination, scale, expressions, view-point, aspect ratio, blur, partial faces, rotation, and alignment.
          To obtain the dataset, please visit <a href="https://github.com/shivram1987/WildSelfieDataset" target="_blank">Project Github Page</a>. 
  
          </p><p><strong>Please cite the following article if you use WSD dataset in your research:</strong></p>
          <p>L. Kumarapu, S. R. Dubey, S. Mukherjee, P. Mohan, S. P. Vinnakoti, and S. Karthikeya, "WSD: Wild Selfie Dataset for Face Recognition in Selfie Images", 
          8th International Conference on Computer Vision and Image Processing (CVIP), Nov 2023.</p>
      </li> <br>
      
      <li><h5><strong>Electromyography Analysis of Human Activities - DataBase 1 (EMAHA-DB1)</strong></h5>
          <p> It is a novel dataset of multi-channel surface electromyography (sEMG) signals to evaluate the activities of daily living (ADL). 
          The dataset is acquired from 25 able-bodied subjects while performing 22 activities categorized according to the functional arm activity 
          behavioral system (FAABOS) (3 - full hand gestures, 6 - open/close office draw, 8 - grasping and holding of small office objects, 
          2 - flexion and extension of finger movements, 2 - writing and 1 - rest).  The dataset can be analyzed for hand activity recognition classification performance.  
          To obtain the dataset, please <a href="https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/R6JJ4Q" target="_blank">Visit Harvard Dataverse - repository</a>. 
  
          </p><p><strong>Please cite the following repository and related article if you validate your method on this dataset:</strong></p>
          <p>N. K. Karnam, A. C. Turlapaty, S. R. Dubey, and B. Gokaraju, “Electromyography Analysis of Human Activities - DataBase 1 (EMAHA-DB1)”, 
              Harvard Dataverse, 2023. (doi: 10.7910/DVN/R6JJ4Q)</p>
          <p>N. K. Karnam, A. C. Turlapaty, S. R. Dubey, and B. Gokaraju, “EMAHA-DB1: A New Upper Limb sEMG Dataset for Classification of Activities of Daily Living”, 
              IEEE Transactions on Instrumentation and Measurement, 2023.</p>
      </li> <br>
      <li><h5><strong>LEDNet Dataset</strong></h5>
          <p>The LEDNet dataset consists of image data of a field area that are captured from a mobile phone camera. 
          Images in the dataset contain the information of an area where a PCB board is placed, containing 6 LEDs. 
          Each state of the LEDs on the PCB board represents a binary number, with the ON state corresponding to binary 1 and the OFF state corresponding to binary 0. 
          All the LEDs placed in sequence represent a binary sequence or encoding of an analog value.
          Dataset consists of image data of an experimental setup collected under different lighting conditions and various heights. 
          To obtain the dataset, please <a href="https://ieee-dataport.org/documents/lednet-data" target="_blank">Visit IEEE Data Port</a>. 
          High-Resolution data can be found at <a href="https://drive.google.com/drive/folders/1EHK5wEg98jVRyU6ORSwQRTU0GT2tdfyD?usp=sharing target=" _blank"="">this drive</a>.</p>
  
          <p><strong>Please cite the following paper if you validate your method on this dataset:</strong></p>
          <p>Nehul Rangappa, Yerra Raja Vara Prasad, and Shiv Ram Dubey, “LEDNet: A Deep Learning Based Ground Sensor Data Monitoring System For Wide Area Precision Agriculture Applications”, 
              IEEE Sensors Journal, 22(1):842-850, Jan 2022.</p>
      </li> <br>
      <li><h5><strong>IIITS MFace Dataset: A Multiface Challenging Dataset for Face Recognition</strong></h5>
          <p>This dataset contains a gallery set and a probe set containing face images of seven subjects captured in unconstrained environment. 
          To obtain the dataset, please <a href="Docs/Agreement_IIITS_MFace_Dataset.pdf" target="_blank">click here</a> and download the agreement form. 
          Please fill it, sign it and send it to the email id: srdubey@iiits.in and srdubey@iiita.ac.in.</p>
  
          <p><strong>Please cite the following paper if you validate your method on this dataset:</strong></p>
          <p>Shiv Ram Dubey and Snehasis Mukherjee, “A Multi-Face Challenging Dataset for Robust Face Recognition”, 
              15th International Conference on Control, Automation, Robotics and Vision (ICARCV), 2018.</p>
      </li>
  </ul>
  
  
  <script language="JavaScript">hideallbibs();</script>
  </div>
  
  <div id="footer" class="container-fluid navbar-fixed-bottom text-center">
    <div class="row">Copyright © Dr. Santosh Kumar</div>
  </div>
  
  
  
  
  
  
  </body><grammarly-desktop-integration data-grammarly-shadow-root="true"></grammarly-desktop-integration></html>